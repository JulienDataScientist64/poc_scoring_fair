# --- Core Python and File System ---
import os
import re
import requests
import importlib.util
import sys
from typing import List, Dict, Any, Optional

# --- Core Data Handling & Computation ---
import pandas as pd
import numpy as np
import joblib

# --- Streamlit (Application Framework) ---
import streamlit as st

# --- Plotting & Visualization ---
import plotly.express as px # R√©introduit pour l'EDA et autres graphiques interactifs

# --- Fairness Libraries ---
# ExponentiatedGradient est import√© dynamiquement si n√©cessaire dans ensure_eowrapper_in_main
from fairlearn.metrics import (
    demographic_parity_difference,
    equalized_odds_difference
)

# --- Scikit-learn Metrics ---
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

# --- CHEMINS ET CONSTANTES ---
RAW_DATA_FILENAME: str = "application_train.csv" # R√©introduit pour l'EDA
MODEL_BASELINE_FILENAME: str = "lgbm_baseline.joblib"
BASELINE_THRESHOLD_FILENAME: str = "baseline_threshold.joblib"
MODEL_WRAPPED_EO_FILENAME: str = "eo_wrapper_with_proba.joblib"
WRAPPER_EO_MODULE_FILENAME: str = "wrapper_eo.py"

# Dictionnaire des artefacts √† t√©l√©charger (r√©introduction de RAW_DATA_FILENAME)
ARTEFACTS: Dict[str, str] = {
    RAW_DATA_FILENAME: "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/application_train.csv",
    BASELINE_THRESHOLD_FILENAME: "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/baseline_threshold.joblib",
    MODEL_WRAPPED_EO_FILENAME: "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/eo_wrapper_with_proba.joblib",
    MODEL_BASELINE_FILENAME: "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/lgbm_baseline.joblib",
    WRAPPER_EO_MODULE_FILENAME: "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/wrapper_eo.py",
    "X_test_pre.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/X_test_pre.parquet",
    "y_test.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/y_test.parquet",
    "A_test.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/A_test.parquet",
}

# -- Streamlit config --
st.set_page_config(
    page_title="POC Scoring √âquitable (Simplifi√©)", # Le titre de la page est important pour l'accessibilit√© (WCAG)
    layout="wide",
    initial_sidebar_state="expanded"
)

# -- Token Hugging Face pour API priv√©e si besoin --
HF_TOKEN: Optional[str] = st.secrets.get("HF_TOKEN", None)
HEADERS: Dict[str, str] = {"Authorization": f"Bearer {HF_TOKEN}"} if HF_TOKEN else {}

def download_if_missing(filename: str, url: str) -> None:
    """T√©l√©charge le fichier depuis Hugging Face si absent localement."""
    if not os.path.exists(filename):
        st.info(f"T√©l√©chargement de {filename}...")
        try:
            with requests.get(url, stream=True, headers=HEADERS) as r:
                r.raise_for_status()
                with open(filename, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
            st.success(f"{filename} t√©l√©charg√©.")
        except Exception as e:
            st.error(f"Erreur lors du t√©l√©chargement de {filename}: {e}")
            if isinstance(e, requests.exceptions.HTTPError) and e.response is not None:
                st.error(f"R√©ponse du serveur: {e.response.status_code} - {e.response.text}")
            st.stop()

for fname, url in ARTEFACTS.items():
    download_if_missing(fname, url)

def ensure_eowrapper_in_main(wrapper_file_path: str = WRAPPER_EO_MODULE_FILENAME) -> Optional[type]:
    """Charge dynamiquement EOWrapper et l'injecte dans __main__."""
    try:
        temp_mod_name = "eowrapper_dyn_simplified_eda" # Nom de module unique
        spec = importlib.util.spec_from_file_location(temp_mod_name, wrapper_file_path)
        if spec is None or spec.loader is None:
            st.error(f"Impossible de cr√©er la spec pour le module depuis {wrapper_file_path}")
            return None
        
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        cls = getattr(module, "EOWrapper", None)
        if cls is None:
            st.error(f"Classe EOWrapper non trouv√©e dans {wrapper_file_path}")
            return None
            
        cls.__module__ = "__main__"
        setattr(sys.modules["__main__"], "EOWrapper", cls)
        
        from fairlearn.reductions import ExponentiatedGradient 
        setattr(sys.modules["__main__"], "ExponentiatedGradient", ExponentiatedGradient)

        return cls
    except Exception as e:
        st.error(f"Erreur lors du chargement dynamique de EOWrapper: {e}")
        st.exception(e)
        return None

@st.cache_data
def load_parquet_file(path: str) -> Optional[pd.DataFrame]:
    """Charge un fichier Parquet."""
    try:
        return pd.read_parquet(path)
    except Exception as e:
        st.error(f"Erreur de chargement du fichier Parquet {path}: {e}")
        return None

@st.cache_data
def load_csv_sample_for_eda(filename: str, sample_frac: float = 0.1, columns: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
    """Charge un √©chantillon d'un fichier CSV pour l'EDA."""
    try:
        df = pd.read_csv(filename, usecols=columns)
        if 0.0 < sample_frac < 1.0:
            if len(df) * sample_frac >= 1:
                df = df.sample(frac=sample_frac, random_state=42)
        return df
    except FileNotFoundError:
        st.error(f"Fichier EDA non trouv√©: {filename}")
        return None
    except Exception as e:
        st.error(f"Erreur de chargement du fichier CSV {filename} pour EDA: {e}")
        return None


@st.cache_resource
def load_model_joblib(path: str) -> Any:
    """Charge un mod√®le sauvegard√© avec joblib."""
    st.info(f"Tentative de chargement du mod√®le depuis {path}...")
    try:
        return joblib.load(path)
    except Exception as e:
        st.error(f"Erreur de chargement du mod√®le {path}: {e}")
        st.exception(e)
        return None

def sanitize_feature_names(df_input: pd.DataFrame) -> pd.DataFrame:
    """Nettoie les noms de colonnes."""
    df = df_input.copy()
    cleaned_columns = [re.sub(r"[^a-zA-Z0-9_]", "_", str(col)) for col in df.columns]
    df.columns = cleaned_columns
    return df

# === Chargement des mod√®les et donn√©es ===
model_baseline = load_model_joblib(MODEL_BASELINE_FILENAME)
optimal_thresh_baseline = load_model_joblib(BASELINE_THRESHOLD_FILENAME)
if optimal_thresh_baseline is None:
    st.warning(f"Seuil baseline ('{BASELINE_THRESHOLD_FILENAME}') non trouv√© ou erreur. Fallback √† 0.5.")
    optimal_thresh_baseline = 0.5
else:
    st.sidebar.info(f"Seuil optimal baseline : {optimal_thresh_baseline:.3f}")

EOWrapper_class = ensure_eowrapper_in_main()
model_eo_wrapper = None
if EOWrapper_class is not None:
    model_eo_wrapper = load_model_joblib(MODEL_WRAPPED_EO_FILENAME)

if model_baseline:
    st.sidebar.success("Mod√®le baseline charg√©.")
if model_eo_wrapper:
    st.sidebar.success("EO Wrapper charg√©.")
    if hasattr(model_eo_wrapper, 'threshold'):
         st.sidebar.info(f"Seuil EO Wrapper : {model_eo_wrapper.threshold:.4f}")
    else:
        st.sidebar.warning("L'objet EO Wrapper charg√© n'a pas d'attribut 'threshold'.")

X_test_raw = load_parquet_file("X_test_pre.parquet")
y_test = load_parquet_file("y_test.parquet")
A_test = load_parquet_file("A_test.parquet")

X_test = None
if X_test_raw is not None:
    X_test = sanitize_feature_names(X_test_raw)
    st.sidebar.info("Donn√©es de test (X_test) nettoy√©es.")
if y_test is not None:
    y_test = y_test.squeeze()
    st.sidebar.info("Donn√©es de test (y_test) charg√©es.")
if A_test is not None:
    A_test = A_test.squeeze()
    st.sidebar.info("Donn√©es de test (A_test) charg√©es.")

# Chargement des donn√©es pour l'EDA
df_eda_sample = load_csv_sample_for_eda(RAW_DATA_FILENAME, sample_frac=0.05) # √âchantillon plus petit pour l'EDA
if df_eda_sample is not None:
    st.sidebar.info("√âchantillon de donn√©es pour EDA charg√©.")


# === Fonctions m√©triques ===
def compute_classification_metrics(
    y_true: pd.Series, 
    y_pred_hard: np.ndarray, 
    y_pred_proba_positive_class: np.ndarray
) -> Dict[str, float]:
    metrics = {}
    try:
        metrics["AUC"] = roc_auc_score(y_true, y_pred_proba_positive_class)
        metrics["Accuracy"] = accuracy_score(y_true, y_pred_hard)
        metrics["Precision (1)"] = precision_score(y_true, y_pred_hard, pos_label=1, zero_division=0)
        metrics["Recall (1)"] = recall_score(y_true, y_pred_hard, pos_label=1, zero_division=0)
        metrics["F1 (1)"] = f1_score(y_true, y_pred_hard, pos_label=1, zero_division=0)
        metrics["Taux de s√©lection"] = np.mean(y_pred_hard)
    except Exception as e:
        st.warning(f"Erreur calcul m√©triques classification: {e}")
        for k in ["AUC", "Accuracy", "Precision (1)", "Recall (1)", "F1 (1)", "Taux de s√©lection"]:
            metrics.setdefault(k, np.nan)
    return metrics

def compute_fairness_metrics(
    y_true: pd.Series, 
    y_pred_hard: np.ndarray, 
    sensitive_features: pd.Series
) -> Dict[str, float]:
    metrics = {}
    try:
        metrics["DPD"] = demographic_parity_difference(y_true, y_pred_hard, sensitive_features=sensitive_features)
        metrics["EOD"] = equalized_odds_difference(y_true, y_pred_hard, sensitive_features=sensitive_features)
    except Exception as e:
        st.warning(f"Erreur calcul m√©triques d'√©quit√©: {e}")
        metrics.setdefault("DPD", np.nan)
        metrics.setdefault("EOD", np.nan)
    return metrics

# === Sidebar navigation (r√©introduit EDA et Prediction Client) ===
st.sidebar.title("üìä POC Scoring √âquitable")
page_options: List[str] = [
    "Analyse Exploratoire (EDA)",
    "R√©sultats & Comparaisons",
    "Pr√©diction sur Client S√©lectionn√©"
]
default_page_index: int = 0

# Utiliser une cl√© de session unique pour √©viter les conflits si d'autres apps utilisent la m√™me
session_key_page_index = "current_page_index_poc_scoring" 
if session_key_page_index not in st.session_state:
    st.session_state[session_key_page_index] = default_page_index

page: str = st.sidebar.radio(
    "Navigation",
    page_options,
    index=st.session_state[session_key_page_index],
    key="nav_radio_poc_scoring" # Cl√© de widget unique
)
if page_options.index(page) != st.session_state[session_key_page_index]:
    st.session_state[session_key_page_index] = page_options.index(page)
    st.rerun()


# === Contenu des Pages ===
if page == "Analyse Exploratoire (EDA)":
    st.header("üîé Analyse Exploratoire des Donn√©es (EDA)")
    st.caption(f"Bas√©e sur un √©chantillon de {len(df_eda_sample) if df_eda_sample is not None else 0} lignes du fichier `{RAW_DATA_FILENAME}`.")

    if df_eda_sample is not None and not df_eda_sample.empty:
        st.subheader("Aper√ßu des donn√©es brutes (√©chantillon)")
        st.dataframe(df_eda_sample.head(), use_container_width=True)

        st.subheader("Statistiques descriptives (variables num√©riques)")
        st.dataframe(df_eda_sample.describe(include=np.number).T, use_container_width=True)
        
        # Comptage de la variable cible 'TARGET'
        if "TARGET" in df_eda_sample.columns:
            st.subheader("Distribution de la variable cible 'TARGET'")
            target_counts = df_eda_sample["TARGET"].value_counts()
            target_counts_percent = df_eda_sample["TARGET"].value_counts(normalize=True) * 100
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("Comptage absolu :")
                st.dataframe(target_counts)
            with col2:
                st.write("Pourcentage :")
                st.dataframe(target_counts_percent.map("{:.2f}%".format))

            # Graphique interactif 1: Histogramme de la variable TARGET
            # WCAG: Titre clair, couleurs contrast√©es par d√©faut de Plotly, texte lisible.
            try:
                fig_target_hist = px.histogram(
                    df_eda_sample, 
                    x="TARGET", 
                    color="TARGET", # Utilisation de la couleur pour distinguer les cat√©gories
                    title="Histogramme de la variable cible 'TARGET'",
                    labels={"TARGET": "Classe de d√©faut (0: Non-d√©faut, 1: D√©faut)"},
                    text_auto=True # Affiche les comptages sur les barres
                )
                fig_target_hist.update_layout(bargap=0.2) # Espace entre les barres
                st.plotly_chart(fig_target_hist, use_container_width=True)
            except Exception as e:
                st.warning(f"Impossible de g√©n√©rer l'histogramme de TARGET: {e}")
        else:
            st.warning("La colonne 'TARGET' n'est pas pr√©sente dans l'√©chantillon de donn√©es pour l'EDA.")

        # Graphique interactif 2: Distribution d'une variable num√©rique (ex: AMT_INCOME_TOTAL)
        # Choix d'une variable num√©rique pertinente pour l'exemple
        numerical_col_for_eda = 'AMT_INCOME_TOTAL' # Revenu total
        if numerical_col_for_eda in df_eda_sample.columns:
            st.subheader(f"Distribution de '{numerical_col_for_eda}'")
            # WCAG: Titre clair, texte lisible.
            # Filtrer les outliers pour une meilleure visualisation (optionnel, mais souvent utile)
            # Par exemple, limiter au 99√®me percentile pour √©viter que quelques valeurs extr√™mes √©crasent le graphique
            income_cap = df_eda_sample[numerical_col_for_eda].quantile(0.99)
            df_filtered_income = df_eda_sample[df_eda_sample[numerical_col_for_eda] < income_cap]

            try:
                fig_income_dist = px.histogram(
                    df_filtered_income, 
                    x=numerical_col_for_eda, 
                    color="TARGET" if "TARGET" in df_filtered_income else None, # Colorer par TARGET si disponible
                    marginal="box", # Ajoute un box plot en marge
                    title=f"Distribution de '{numerical_col_for_eda}' (plafonn√© √† {income_cap:,.0f})",
                    labels={numerical_col_for_eda: "Revenu total", "TARGET": "Classe de d√©faut"}
                )
                st.plotly_chart(fig_income_dist, use_container_width=True)
            except Exception as e:
                st.warning(f"Impossible de g√©n√©rer l'histogramme de {numerical_col_for_eda}: {e}")
        else:
            st.info(f"La colonne '{numerical_col_for_eda}' n'est pas disponible pour l'EDA.")
            
    else:
        st.error("L'√©chantillon de donn√©es pour l'EDA n'a pas pu √™tre charg√©. V√©rifiez les logs.")


elif page == "R√©sultats & Comparaisons":
    st.header("üìä R√©sultats comparatifs sur jeu de test")
    # Le titre de la page est d√©j√† d√©fini via st.set_page_config, mais un header est bon pour la structure.
    if model_baseline is not None and model_eo_wrapper is not None and \
       X_test is not None and y_test is not None and A_test is not None and \
       optimal_thresh_baseline is not None:
        try:
            # st.header("Comparaison des mod√®les sur le jeu de test") # D√©j√† un header de page
            
            y_test_proba_b: np.ndarray = model_baseline.predict_proba(X_test)[:, 1]
            y_test_pred_b: np.ndarray = (y_test_proba_b >= optimal_thresh_baseline).astype(int)
            metrics_b: Dict[str, float] = compute_classification_metrics(y_test, y_test_pred_b, y_test_proba_b)
            fairness_b: Dict[str, float] = compute_fairness_metrics(y_test, y_test_pred_b, A_test)

            y_test_proba_eo: np.ndarray = model_eo_wrapper.predict_proba(X_test)[:, 1]
            y_test_pred_eo: np.ndarray = model_eo_wrapper.predict(X_test) 
            metrics_eo: Dict[str, float] = compute_classification_metrics(y_test, y_test_pred_eo, y_test_proba_eo)
            fairness_eo: Dict[str, float] = compute_fairness_metrics(y_test, y_test_pred_eo, A_test)

            df_res = pd.DataFrame([
                {"Mod√®le": "Baseline", **metrics_b, **fairness_b},
                {"Mod√®le": "EO Wrapper", **metrics_eo, **fairness_eo}
            ])
            # WCAG: Les tables de donn√©es doivent √™tre correctement structur√©es. st.dataframe le fait.
            # Le contraste des couleurs du th√®me par d√©faut de Streamlit est g√©n√©ralement bon.
            st.dataframe(df_res.set_index("Mod√®le").style.format("{:.3f}", na_rep="-"), use_container_width=True)
        
        except Exception as e:
            st.error(f"Erreur lors du calcul ou de l'affichage des r√©sultats comparatifs: {e}")
            st.exception(e)
    else:
        st.warning("Des √©l√©ments sont manquants pour afficher les r√©sultats. V√©rifiez les messages dans la barre lat√©rale.")
        # Messages d'erreur plus sp√©cifiques pour aider au d√©bogage
        if model_baseline is None: st.error("- Mod√®le baseline non charg√©.")
        if model_eo_wrapper is None: st.error("- Mod√®le EO wrapper non charg√©.")
        if X_test is None: st.error("- Donn√©es X_test non charg√©es.")
        if y_test is None: st.error("- Donn√©es y_test non charg√©es.")
        if A_test is None: st.error("- Donn√©es A_test (features sensibles) non charg√©es.")
        if optimal_thresh_baseline is None: st.error("- Seuil optimal baseline non charg√©.")

elif page == "Pr√©diction sur Client S√©lectionn√©":
    st.header("üîç Pr√©diction sur un Client S√©lectionn√© du Jeu de Test")

    if X_test is not None and model_baseline is not None and model_eo_wrapper is not None \
       and optimal_thresh_baseline is not None and y_test is not None:
        
        # S√©lection du client
        # WCAG: Le widget selectbox est accessible au clavier.
        client_ids = X_test.index.tolist()
        if not client_ids:
            st.warning("Aucun ID client disponible dans le jeu de test.")
        else:
            # Limiter le nombre d'options pour la performance du selectbox si X_test est tr√®s grand
            max_clients_in_selectbox = 2000 
            if len(client_ids) > max_clients_in_selectbox:
                st.info(f"Affichage des {max_clients_in_selectbox} premiers IDs clients pour la s√©lection.")
                client_ids_to_display = client_ids[:max_clients_in_selectbox]
            else:
                client_ids_to_display = client_ids

            selected_client_id_str = st.selectbox(
                "Choisissez un ID client du jeu de test:",
                options=[str(id_val) for id_val in client_ids_to_display] # Convertir en str pour le selectbox
            )
            
            # Convertir l'ID s√©lectionn√© (str) au type original de l'index
            selected_client_id: Any
            try:
                if X_test.index.dtype == 'int64' or X_test.index.dtype == 'int32':
                    selected_client_id = int(selected_client_id_str)
                elif X_test.index.dtype == 'float64' or X_test.index.dtype == 'float32':
                     selected_client_id = float(selected_client_id_str)
                else: # Conserver comme string si ce n'est ni int ni float
                    selected_client_id = selected_client_id_str
            except ValueError:
                st.error(f"ID client '{selected_client_id_str}' invalide.")
                st.stop()


            if selected_client_id in X_test.index:
                client_features = X_test.loc[[selected_client_id]] # Doit √™tre un DataFrame pour predict_proba
                client_true_target = y_test.loc[selected_client_id] if selected_client_id in y_test.index else "Inconnue"

                st.subheader(f"Donn√©es du client ID: {selected_client_id}")
                st.write(f"Vraie cible (TARGET) : **{client_true_target}**")
                st.dataframe(client_features.T.rename(columns={0: "Valeur"}), use_container_width=True)

                # Pr√©dictions
                try:
                    proba_baseline = model_baseline.predict_proba(client_features)[0, 1]
                    pred_baseline = (proba_baseline >= optimal_thresh_baseline).astype(int)

                    proba_eo = model_eo_wrapper.predict_proba(client_features)[0, 1]
                    pred_eo = model_eo_wrapper.predict(client_features)[0] # predict() du wrapper applique le seuil

                    st.subheader("R√©sultats de la Pr√©diction")
                    results_data = {
                        "M√©trique": ["Probabilit√© de d√©faut (classe 1)", "Pr√©diction (0 ou 1)"],
                        "Mod√®le Baseline": [f"{proba_baseline:.4f}", pred_baseline],
                        "Mod√®le EO Wrapper": [f"{proba_eo:.4f}", pred_eo]
                    }
                    df_pred_results = pd.DataFrame(results_data)
                    st.table(df_pred_results.set_index("M√©trique")) # st.table est bon pour l'accessibilit√© des petites tables

                except Exception as e_pred:
                    st.error(f"Erreur lors de la pr√©diction pour le client {selected_client_id}: {e_pred}")
            else:
                st.error(f"L'ID client {selected_client_id} n'a pas √©t√© trouv√© dans le jeu de test apr√®s conversion.")
    else:
        st.warning("Des √©l√©ments sont manquants pour effectuer une pr√©diction : mod√®les ou donn√©es de test.")

st.sidebar.markdown("---")
st.sidebar.caption("Version simplifi√©e avec EDA et pr√©diction client.")

