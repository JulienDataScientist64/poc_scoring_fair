import os
import re
import requests
import importlib.util

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.graph_objects as go
import plotly.express as px
import shap
import dalex as dx

from fairlearn.reductions import ExponentiatedGradient
from fairlearn.metrics import (
    MetricFrame,
    selection_rate as fairlearn_selection_rate,
    demographic_parity_difference,
    equalized_odds_difference
)
from sklearn.metrics import (
    roc_auc_score,
    roc_curve,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

# === Chemins & artefacts Hugging Face ===
ARTEFACTS = {
    "application_train.csv": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/application_train.csv",
    "baseline_threshold.joblib": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/baseline_threshold.joblib",
    "eo_wrapper_with_proba.joblib": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/eo_wrapper_with_proba.joblib",
    "lgbm_baseline.joblib": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/lgbm_baseline.joblib",
    "wrapper_eo.py": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/wrapper_eo.py",
    "X_valid_pre.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/X_valid_pre.parquet",
    "y_valid.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/y_valid.parquet",
    "A_valid.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/A_valid.parquet",
    "X_test_pre.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/X_test_pre.parquet",
    "y_test.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/y_test.parquet",
    "A_test.parquet": "https://huggingface.co/cantalapiedra/poc_scoring_fair/resolve/main/A_test.parquet"
}

def download_if_missing(filename, url):
    """T√©l√©charge le fichier depuis Hugging Face si absent localement."""
    if not os.path.exists(filename):
        st.info(f"T√©l√©chargement de {filename} depuis Hugging Face...")
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(filename, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
            st.success(f"{filename} t√©l√©charg√©.")
        except Exception as e:
            st.error(f"Erreur lors du t√©l√©chargement de {filename}: {e}")
            st.stop()

# --- Download all needed artefacts from Hugging Face ---
for fname, url in ARTEFACTS.items():
    download_if_missing(fname, url)

# === Import dynamique de la classe EOWrapper ===
spec = importlib.util.spec_from_file_location("wrapper_eo", "wrapper_eo.py")
wrapper_eo = importlib.util.module_from_spec(spec)
spec.loader.exec_module(wrapper_eo)
EOWrapper = wrapper_eo.EOWrapper

# === D√©finition des chemins ===
RAW_DATA_FILENAME = "application_train.csv"
MODEL_BASELINE_FILENAME = "lgbm_baseline.joblib"
BASELINE_THRESHOLD_FILENAME = "baseline_threshold.joblib"
MODEL_WRAPPED_EO_FILENAME = "eo_wrapper_with_proba.joblib"
SPLITS_DIR = ""  # splits √† la racine

# === Page config Streamlit ===
st.set_page_config(
    page_title="POC Scoring Cr√©dit √âquitable",
    layout="wide",
    initial_sidebar_state="expanded"
)

# === Fonctions de chargement / cache Streamlit ===
@st.cache_data
def load_parquet_file(path):
    return pd.read_parquet(path)

@st.cache_data
def load_csv_sample(filename, sample_frac=0.3):
    df = pd.read_csv(filename)
    if sample_frac < 1.0 and len(df) * sample_frac >= 1:
        df = df.sample(frac=sample_frac, random_state=42)
    return df

@st.cache_resource
def load_model(path):
    return joblib.load(path)

def sanitize_feature_names(df_input):
    df = df_input.copy()
    new_columns = []
    counts = {}
    for col in df.columns:
        new_col = str(col)
        new_col = re.sub(r"[^A-Za-z0-9_.-]+", "_", new_col)
        if new_col in counts:
            counts[new_col] += 1
            final_col = f"{new_col}_{counts[new_col]}"
        else:
            counts[new_col] = 0
            final_col = new_col
        new_columns.append(final_col)
    df.columns = new_columns
    return df

# === Chargement effectif des artefacts ===
try:
    model_baseline = load_model(MODEL_BASELINE_FILENAME)
    st.sidebar.success("Mod√®le baseline charg√© !")
except Exception as e:
    st.error(f"Erreur de chargement du mod√®le baseline : {e}")
    st.stop()
try:
    optimal_thresh_baseline = joblib.load(BASELINE_THRESHOLD_FILENAME)
    st.sidebar.info(f"Seuil optimal baseline : {optimal_thresh_baseline:.3f}")
except Exception as e:
    st.warning(f"Seuil baseline non trouv√©, fallback 0.5 ({e})")
    optimal_thresh_baseline = 0.5
try:
    model_eo_wrapper = load_model(MODEL_WRAPPED_EO_FILENAME)
    assert isinstance(model_eo_wrapper, EOWrapper)
    st.sidebar.success("EO Wrapper charg√© !")
except Exception as e:
    st.error(f"Erreur de chargement du mod√®le EO Wrapper : {e}")
    st.stop()

try:
    X_valid = load_parquet_file("X_valid_pre.parquet")
    y_valid = load_parquet_file("y_valid.parquet").squeeze()
    A_valid = load_parquet_file("A_valid.parquet").squeeze()
    X_test = load_parquet_file("X_test_pre.parquet")
    y_test = load_parquet_file("y_test.parquet").squeeze()
    A_test = load_parquet_file("A_test.parquet").squeeze()
except Exception as e:
    st.error(f"Erreur de chargement des splits de donn√©es : {e}")
    st.stop()

X_valid = sanitize_feature_names(X_valid)
X_test = sanitize_feature_names(X_test)

# EDA brute pour analyse
try:
    df_eda_raw_sample = load_csv_sample(RAW_DATA_FILENAME, sample_frac=0.3)
except Exception:
    df_eda_raw_sample = None

# === Fonctions m√©triques ===
def compute_classification_metrics(y_true, y_pred_hard, y_pred_proba_positive_class):
    return {
        "AUC": roc_auc_score(y_true, y_pred_proba_positive_class),
        "Accuracy": accuracy_score(y_true, y_pred_hard),
        "Precision (1)": precision_score(y_true, y_pred_hard, zero_division=0),
        "Recall (1)": recall_score(y_true, y_pred_hard, zero_division=0),
        "F1 (1)": f1_score(y_true, y_pred_hard, zero_division=0),
        "Taux de s√©lection": np.mean(y_pred_hard),
    }

def compute_fairness_metrics(y_true, y_pred_hard, sensitive_features):
    try:
        dpd = demographic_parity_difference(y_true, y_pred_hard, sensitive_features=sensitive_features)
        eod = equalized_odds_difference(y_true, y_pred_hard, sensitive_features=sensitive_features)
        return {"DPD": dpd, "EOD": eod}
    except Exception:
        return {"DPD": np.nan, "EOD": np.nan}

# === Sidebar navigation ===
st.sidebar.title("üìä POC Scoring √âquitable")
page_options = [
    "Contexte & Objectifs", "M√©thodologie", "Analyse Exploratoire", "R√©sultats & Comparaisons",
    "ROC/Proba - Baseline", "ROC/Proba - EO Wrapper",
    "Intersectionnalit√©", "Explicabilit√© Locale", "Explicabilit√© Globale"
]
page = st.sidebar.radio("Navigation", page_options, index=0)
st.sidebar.markdown("---")
st.sidebar.info(f"Seuil Baseline‚ÄØ: {optimal_thresh_baseline:.4f}")
st.sidebar.info(f"Seuil EO Wrapper‚ÄØ: {model_eo_wrapper.threshold:.4f}")

# === Page Content ===

if page == "Contexte & Objectifs":
    st.header("Contexte & R√©f√©rences")
    st.markdown(
        """
        **Pourquoi l‚Äô√©quit√© dans le scoring cr√©dit ?**
        - Les r√©gulateurs (comme l‚ÄôIA Act et les lois anti-discrimination) imposent que les mod√®les de scoring cr√©dit n‚Äôavantagent ni ne d√©savantagent un groupe (par exemple le genre).
        - Ce POC compare deux approches :
          1. **LightGBM classique** (mod√®le standard de machine learning)
          2. **LightGBM associ√© √† Fairlearn EG-EO** (ajout d‚Äôune contrainte d‚Äô√©quit√© sur la pr√©diction)

        **Objectif m√©tier :**
        Obtenir un mod√®le performant mais qui reste juste entre les diff√©rents groupes (ex‚ÄØ: hommes/femmes).
        """
    )

    st.subheader("Papiers de r√©f√©rence")
    with st.expander("Hardt, Price & Srebro (2016) ‚Äì Equalized Odds"):
        st.write(
            """
            **R√©sum√© p√©dagogique :**
            - Equalized Odds impose que le taux de bonne d√©tection (rappel) soit similaire pour chaque groupe (par exemple hommes et femmes), pour les personnes qui remboursent ou non.
            - Un mod√®le respectant bien Equalized Odds limite donc les √©carts d‚Äôerreur selon le groupe sensible.
            """
        )
        st.markdown("[Lire le papier (arXiv)](https://arxiv.org/abs/1610.02413)")

    with st.expander("Agarwal et al. (2019) ‚Äì Exponentiated Gradient"):
        st.write(
            """
            **R√©sum√© p√©dagogique :**
            - L‚Äôalgorithme Exponentiated Gradient combine plusieurs mod√®les en ajustant leurs poids pour trouver un compromis optimal entre performance et √©quit√©.
            - √Ä chaque √©tape, il renforce les mod√®les qui respectent le mieux la contrainte d‚Äô√©quit√©.
            - Cette m√©thode permet d‚Äôobtenir un mod√®le global qui ne discrimine pas, tout en gardant un bon niveau de pr√©diction.
            """
        )
        st.markdown("[Lire le papier (ACM)](https://dl.acm.org/doi/10.1145/3287560.3287572)")

    st.subheader("M√©triques d'√©quit√© utilis√©es")
    st.markdown(
        """
        - **Demographic Parity Difference (DPD) :**
          > Mesure la diff√©rence de taux d‚Äôattribution positive du cr√©dit entre groupes (id√©al‚ÄØ: z√©ro diff√©rence).
        - **Equalized Odds Difference (EOD) :**
          > Mesure l‚Äô√©cart de performance du mod√®le (sensibilit√©/sp√©cificit√©) selon le groupe sensible. Un mod√®le √©quitable aura un EOD proche de z√©ro.
        - **Exponentiated Gradient (EG) :**
          > M√©thode pour trouver un compromis entre performance et √©quit√©, en combinant plusieurs mod√®les de fa√ßon intelligente.
        """
    )

elif page == "M√©thodologie":
    st.header("M√©thodologie")

    st.subheader("Donn√©es & Pr√©paration")
    st.write(
        """
        - **Jeu de donn√©es** : Home Credit (~300 000 clients, 120 variables socio-√©conomiques).
        - D√©coupage en 3 parties : apprentissage (80%), validation (10%), test (10%).
        - **Nettoyage** : gestion des valeurs bizarres ou manquantes, suppression des doublons, filtrage sur le genre, plafonnement des revenus extr√™mes.
        - **Nouvelles variables** : cr√©ation de ratios simples (ex‚ÄØ: mensualit√©/revenu, cr√©dit/revenu), transformation de l‚Äô√¢ge.
        - **Mise en forme** : transformation des variables cat√©gorielles, d√©coupage de l‚Äô√¢ge en tranches, etc.
        - **Encodage & imputation** : gestion automatique des valeurs manquantes et transformation des variables pour les mod√®les.
        """
    )

    st.subheader("Mod√®le de base (LightGBM)")
    st.write(
        """
        - Mod√®le classique de machine learning qui apprend √† pr√©dire le d√©faut de remboursement.
        - Prend en compte le d√©s√©quilibre entre bons et mauvais payeurs.
        - Le seuil de d√©cision (pour dire ‚Äúd√©faut‚Äù ou ‚Äúpas d√©faut‚Äù) est choisi de fa√ßon optimale sur la validation.
        """
    )

    st.subheader("Mod√®le √©quitable (EG-EO)")
    st.write(
        """
        - Mod√®le LightGBM ajust√© avec Fairlearn pour garantir l‚Äô√©quit√© entre hommes et femmes.
        - La m√©thode combine plusieurs mod√®les et ajuste leurs poids pour minimiser les √©carts de traitement selon le genre.
        - On fixe une tol√©rance maximale sur l‚Äô√©cart d‚Äô√©quit√© autoris√©.
        """
    )

    st.subheader("√âvaluation et comparaison")
    st.write(
        """
        - **Performances mesur√©es** : capacit√© √† bien trier les clients (AUC, pr√©cision, rappel, F1).
        - **√âquit√©** : on v√©rifie que le mod√®le ne favorise pas un groupe par rapport √† l‚Äôautre, via des m√©triques sp√©cifiques.
        - **Analyse d√©taill√©e** : matrice de confusion et taux de s√©lection par groupe.
        """
    )

elif page == "Analyse Exploratoire":
    st.header("üîé Analyse exploratoire (EDA)")
    if df_eda_raw_sample is not None:
        st.subheader("Aper√ßu √©chantillon")
        st.dataframe(df_eda_raw_sample.head(20), use_container_width=True)
        st.subheader("Statistiques descriptives")
        st.dataframe(df_eda_raw_sample.describe(include=np.number).T, use_container_width=True)
        if "TARGET" in df_eda_raw_sample:
            st.subheader("Distribution de la cible (TARGET)")
            tab = pd.DataFrame(df_eda_raw_sample["TARGET"].value_counts()).T
            st.dataframe(tab, use_container_width=True)
            fig = px.histogram(df_eda_raw_sample, x="TARGET", color="TARGET",
                               color_discrete_sequence=px.colors.qualitative.Safe)
            st.plotly_chart(fig, use_container_width=True)

elif page == "R√©sultats & Comparaisons":
    st.header("üìä R√©sultats comparatifs sur jeu test")
    # Baseline
    y_test_proba_b = model_baseline.predict_proba(X_test)[:, 1]
    y_test_pred_b = (y_test_proba_b >= optimal_thresh_baseline).astype(int)
    metrics_b = compute_classification_metrics(y_test, y_test_pred_b, y_test_proba_b)
    fairness_b = compute_fairness_metrics(y_test, y_test_pred_b, A_test)
    # EO
    y_test_proba_eo = model_eo_wrapper.predict_proba(X_test)[:, 1]
    y_test_pred_eo = model_eo_wrapper.predict(X_test)
    metrics_eo = compute_classification_metrics(y_test, y_test_pred_eo, y_test_proba_eo)
    fairness_eo = compute_fairness_metrics(y_test, y_test_pred_eo, A_test)
    df_res = pd.DataFrame([
        {"Mod√®le": "Baseline", **metrics_b, **fairness_b},
        {"Mod√®le": "EO Wrapper", **metrics_eo, **fairness_eo}
    ])
    st.dataframe(df_res.style.format("{:.3f}"), use_container_width=True)

elif page == "ROC/Proba - Baseline":
    st.header("Courbe ROC & Distribution de probas - Baseline")
    y_val_proba = model_baseline.predict_proba(X_valid)[:, 1]
    fpr, tpr, _ = roc_curve(y_valid, y_val_proba)
    auc_b = roc_auc_score(y_valid, y_val_proba)
    fig_roc = go.Figure()
    fig_roc.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={auc_b:.3f})'))
    fig_roc.add_shape(type='line', x0=0, x1=1, y0=0, y1=1, line=dict(dash='dash'))
    fig_roc.update_layout(title="Courbe ROC (Validation, Baseline)",
                          xaxis_title="Faux positifs (FPR)",
                          yaxis_title="Vrais positifs (TPR)")
    st.plotly_chart(fig_roc, use_container_width=True)
    # Probas
    st.subheader("Distribution des probabilit√©s (Baseline)")
    df_dist = pd.DataFrame({"proba": y_val_proba, "y": y_valid})
    fig_dist = px.histogram(df_dist, x="proba", color="y", nbins=50, barmode='overlay',
                            color_discrete_sequence=px.colors.qualitative.Safe,
                            labels={"y": "Cible", "proba": "Score Baseline"})
    fig_dist.add_vline(x=optimal_thresh_baseline, line_color="red", line_dash="dash")
    st.plotly_chart(fig_dist, use_container_width=True)

elif page == "ROC/Proba - EO Wrapper":
    st.header("Courbe ROC & Distribution de probas - EO Wrapper")
    y_val_proba = model_eo_wrapper.predict_proba(X_valid)[:, 1]
    fpr, tpr, _ = roc_curve(y_valid, y_val_proba)
    auc_eo = roc_auc_score(y_valid, y_val_proba)
    fig_roc = go.Figure()
    fig_roc.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={auc_eo:.3f})'))
    fig_roc.add_shape(type='line', x0=0, x1=1, y0=0, y1=1, line=dict(dash='dash'))
    fig_roc.update_layout(title="Courbe ROC (Validation, EO Wrapper)",
                          xaxis_title="Faux positifs (FPR)",
                          yaxis_title="Vrais positifs (TPR)")
    st.plotly_chart(fig_roc, use_container_width=True)
    # Probas
    st.subheader("Distribution des probabilit√©s (EO Wrapper)")
    df_dist = pd.DataFrame({"proba": y_val_proba, "y": y_valid})
    fig_dist = px.histogram(df_dist, x="proba", color="y", nbins=50, barmode='overlay',
                            color_discrete_sequence=px.colors.qualitative.Safe,
                            labels={"y": "Cible", "proba": "Score EO"})
    fig_dist.add_vline(x=model_eo_wrapper.threshold, line_color="red", line_dash="dash")
    st.plotly_chart(fig_dist, use_container_width=True)

elif page == "Intersectionnalit√©":
    st.header("√âquit√© intersectionnelle")
    # Attribut sensible crois√© genre + √¢ge bin
    df_test_raw = load_csv_sample(RAW_DATA_FILENAME, sample_frac=1.0)
    if "DAYS_BIRTH" in df_test_raw:
        df_test_raw = df_test_raw.assign(AGE_BIN=pd.cut(
            -df_test_raw["DAYS_BIRTH"]/365,
            bins=[17, 25, 35, 45, 55, 65, 100],
            labels=["18-25", "26-35", "36-45", "46-55", "56-65", "66+"]
        ))
    if "CODE_GENDER" in df_test_raw and "AGE_BIN" in df_test_raw:
        # Match index avec X_test
        idx_test = X_test.index.intersection(df_test_raw.index)
        group = df_test_raw.loc[idx_test, "CODE_GENDER"].astype(str) + " | " + df_test_raw.loc[idx_test, "AGE_BIN"].astype(str)
        y_pred_b = (model_baseline.predict_proba(X_test.loc[idx_test])[:, 1] >= optimal_thresh_baseline).astype(int)
        y_pred_eo = model_eo_wrapper.predict(X_test.loc[idx_test])
        metric_inter = MetricFrame(
            metrics={
                "Selection Rate": fairlearn_selection_rate,
                "Recall": lambda y, y_p: recall_score(y, y_p, pos_label=1, zero_division=0),
            },
            y_true=y_test.loc[idx_test],
            y_pred=y_pred_b,
            sensitive_features=group,
        )
        metric_inter_eo = MetricFrame(
            metrics={
                "Selection Rate": fairlearn_selection_rate,
                "Recall": lambda y, y_p: recall_score(y, y_p, pos_label=1, zero_division=0),
            },
            y_true=y_test.loc[idx_test],
            y_pred=y_pred_eo,
            sensitive_features=group,
        )
        # Tableaux
        df_plot = pd.DataFrame({
            "Groupe": metric_inter.by_group.index,
            "Selection Rate - Baseline": metric_inter.by_group["Selection Rate"].values,
            "Recall - Baseline": metric_inter.by_group["Recall"].values,
            "Selection Rate - EO": metric_inter_eo.by_group["Selection Rate"].values,
            "Recall - EO": metric_inter_eo.by_group["Recall"].values
        })
        st.dataframe(df_plot, use_container_width=True)
        # Barplots
        fig = px.bar(df_plot, y="Groupe", x=["Selection Rate - Baseline", "Selection Rate - EO"], barmode="group",
                     title="Taux de s√©lection par groupe (Baseline vs EO)",
                     color_discrete_sequence=px.colors.qualitative.Safe)
        st.plotly_chart(fig, use_container_width=True)
        fig2 = px.bar(df_plot, y="Groupe", x=["Recall - Baseline", "Recall - EO"], barmode="group",
                      title="Recall par groupe (Baseline vs EO)",
                      color_discrete_sequence=px.colors.qualitative.Safe)
        st.plotly_chart(fig2, use_container_width=True)
    else:
        st.warning("Impossible de construire les groupes intersectionnels (DAYS_BIRTH ou CODE_GENDER manquant)")

elif page == "Explicabilit√© Locale":
    st.header("Explicabilit√© locale (SHAP Force Plot)")
    st.caption("Affiche l'explication d'une pr√©diction pour un client s√©lectionn√©.")
    idx_options = X_test.index.astype(str).tolist()[:1000]
    idx_selected = st.selectbox("ID Client √† expliquer :", idx_options, index=0)
    idx_selected = int(idx_selected)
    client_feat = X_test.loc[[idx_selected]]
    # Baseline
    explainer_b = shap.TreeExplainer(model_baseline)
    shap_val_b = explainer_b.shap_values(client_feat)
    expected_val_b = explainer_b.expected_value
    st.markdown("**Force Plot ‚Äì Mod√®le Baseline**")
    plot_html = shap.force_plot(
        expected_val_b[1] if isinstance(expected_val_b, list) else expected_val_b,
        shap_val_b[1][0] if isinstance(shap_val_b, list) else shap_val_b[0],
        client_feat, matplotlib=False
    ).html()
    st.components.v1.html(f"<head>{shap.getjs()}</head><body>{plot_html}</body>", height=220, scrolling=True)
    # EO Wrapper (premier estimateur)
    explainer_eo = shap.TreeExplainer(model_eo_wrapper.mitigator.predictors_[0])
    shap_val_eo = explainer_eo.shap_values(client_feat)
    expected_val_eo = explainer_eo.expected_value
    st.markdown("**Force Plot ‚Äì EO Wrapper (1er estimateur)**")
    plot_html_eo = shap.force_plot(
        expected_val_eo[1] if isinstance(expected_val_eo, list) else expected_val_eo,
        shap_val_eo[1][0] if isinstance(shap_val_eo, list) else shap_val_eo[0],
        client_feat, matplotlib=False
    ).html()
    st.components.v1.html(f"<head>{shap.getjs()}</head><body>{plot_html_eo}</body>", height=220, scrolling=True)

elif page == "Explicabilit√© Globale":
    st.header("Explicabilit√© globale (SHAP - DALEX)")
    # SHAP global baseline
    st.subheader("SHAP - Importance globale Baseline")
    sample_size = min(500, X_valid.shape[0])
    X_sample = X_valid.sample(n=sample_size, random_state=42)
    explainer_b = shap.TreeExplainer(model_baseline)
    shap_val_b = explainer_b.shap_values(X_sample)
    feature_importances = np.abs(shap_val_b[1] if isinstance(shap_val_b, list) else shap_val_b).mean(axis=0)
    df_shap_b = pd.DataFrame({
        "Feature": X_sample.columns,
        "Importance (mean|SHAP|)": feature_importances
    }).sort_values("Importance (mean|SHAP|)", ascending=False).head(20)
    st.dataframe(df_shap_b, use_container_width=True)
    fig = px.bar(df_shap_b, x="Importance (mean|SHAP|)", y="Feature", orientation="h",
                     title="Importance globale Baseline (SHAP)", color="Importance (mean|SHAP|)",
                     color_continuous_scale=px.colors.sequential.Plasma)
    st.plotly_chart(fig, use_container_width=True)
    # SHAP global EO (1er estimateur)
    st.subheader("SHAP - Importance globale EO Wrapper (1er estimateur)")
    explainer_eo = shap.TreeExplainer(model_eo_wrapper.mitigator.predictors_[0])
    shap_val_eo = explainer_eo.shap_values(X_sample)
    feature_importances_eo = np.abs(shap_val_eo[1] if isinstance(shap_val_eo, list) else shap_val_eo).mean(axis=0)
    df_shap_eo = pd.DataFrame({
        "Feature": X_sample.columns,
        "Importance (mean|SHAP|)": feature_importances_eo
    }).sort_values("Importance (mean|SHAP|)", ascending=False).head(20)
    st.dataframe(df_shap_eo, use_container_width=True)
    fig2 = px.bar(df_shap_eo, x="Importance (mean|SHAP|)", y="Feature", orientation="h",
                      title="Importance globale EO Wrapper (SHAP)", color="Importance (mean|SHAP|)",
                      color_continuous_scale=px.colors.sequential.Plasma)
    st.plotly_chart(fig2, use_container_width=True)

    # DALEX
    st.subheader("DALEX - Dropout Loss (AUC)")
    exp_b = dx.Explainer(model_baseline, X_sample, y_valid.loc[X_sample.index], label="Baseline", verbose=False)
    loss_b = exp_b.model_parts(loss_function="auc")
    exp_eo = dx.Explainer(model_eo_wrapper.mitigator.predictors_[0], X_sample, y_valid.loc[X_sample.index], label="EO Wrapper", verbose=False)
    loss_eo = exp_eo.model_parts(loss_function="auc")
    df_dalex = pd.DataFrame({
        "Feature": loss_b.result.variable,
        "Dropout_loss Baseline": loss_b.result.dropout_loss,
        "Dropout_loss EO": loss_eo.result.dropout_loss
    }).query("Feature != '_full_model_'").sort_values("Dropout_loss Baseline", ascending=False).head(20)
    st.dataframe(df_dalex, use_container_width=True)
    fig_dx = px.bar(df_dalex, x="Dropout_loss Baseline", y="Feature", orientation="h",
                     title="DALEX Dropout Loss (Baseline)",
                     color="Dropout_loss Baseline", color_continuous_scale=px.colors.sequential.Viridis)
    st.plotly_chart(fig_dx, use_container_width=True)
    fig_dx2 = px.bar(df_dalex, x="Dropout_loss EO", y="Feature", orientation="h",
                      title="DALEX Dropout Loss (EO Wrapper)",
                      color="Dropout_loss EO", color_continuous_scale=px.colors.sequential.Viridis)
    st.plotly_chart(fig_dx2, use_container_width=True)

st.markdown("---")
st.caption(f"POC Scoring Cr√©dit √âquitable ‚Äì {pd.Timestamp.now(tz='Europe/Paris').strftime('%d/%m/%Y %H:%M')}")